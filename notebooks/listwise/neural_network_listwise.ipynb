{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import configuration as config\n",
    "from src.pipeline.evaluation.evaluation_utils import custom_train_test_split\n",
    "from src.models.listwise_neural_network import sample_listwise, RankingModel\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = config.load_traindata_for_pointwise()\n",
    "df = df.drop(columns=['cv_score'])\n",
    "X_train, X_test, y_train, y_test = custom_train_test_split(df, factors=[\"dataset\", \"model\", \"tuning\", \"scoring\"], target=\"rank\")\n",
    "df = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder      object\n",
      "rank        float64\n",
      "features     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "# train data\n",
    "df['dataset'] = df['dataset'].astype(str)\n",
    "df['features'] = df['dataset'].astype(str) + ' ' + df['model'] + ' ' + df['tuning'] + ' ' + df['scoring']\n",
    "df = df.drop(columns=['dataset', 'model', 'tuning', 'scoring'])\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "# test data\n",
    "df_test['dataset'] = df_test['dataset'].astype(str)\n",
    "df_test['features'] = df_test['dataset'].astype(str) + ' ' + df_test['model'] + ' ' + df_test['tuning'] + ' ' + df_test['scoring']\n",
    "df_test = df_test.drop(columns=['dataset', 'model', 'tuning', 'scoring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9065, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder</th>\n",
       "      <th>rank</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BE</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1114 KNC no F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUCV10RGLMME</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1114 KNC no F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUCV10TE</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1114 KNC no F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUCV2RGLMME</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1114 KNC no F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUCV2TE</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1114 KNC no F1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        encoder  rank        features\n",
       "0            BE  21.0  1114 KNC no F1\n",
       "1  BUCV10RGLMME  19.0  1114 KNC no F1\n",
       "2      BUCV10TE  26.0  1114 KNC no F1\n",
       "3   BUCV2RGLMME  12.0  1114 KNC no F1\n",
       "4       BUCV2TE  28.0  1114 KNC no F1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder\n",
       "OE              869\n",
       "DTEM10          867\n",
       "DTEM5           867\n",
       "CV5TE           866\n",
       "TE              865\n",
       "BUCV2TE         865\n",
       "CBE             865\n",
       "CV10TE          865\n",
       "CV2TE           865\n",
       "DTEM2           864\n",
       "CE              863\n",
       "DE              863\n",
       "WOEE            861\n",
       "BE              857\n",
       "BUCV5TE         857\n",
       "PBTE001         852\n",
       "PBTE01          849\n",
       "BUCV10TE        847\n",
       "ME10E           844\n",
       "ME01E           842\n",
       "ME1E            840\n",
       "CV2RGLMME       839\n",
       "RGLMME          837\n",
       "BUCV2RGLMME     836\n",
       "CV5RGLMME       833\n",
       "CV10RGLMME      827\n",
       "BUCV5RGLMME     822\n",
       "BUCV10RGLMME    819\n",
       "PBTE0001        800\n",
       "OHE             795\n",
       "MHE             779\n",
       "SE              769\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['encoder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "df_tf = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "df_tf_test = tf.data.Dataset.from_tensor_slices(dict(df_test))\n",
    "print(type(df_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listwise = sample_listwise(df_tf)\n",
    "df_listwise_test = sample_listwise(df_tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec={'features': TensorSpec(shape=(), dtype=tf.string, name=None), 'encoder': TensorSpec(shape=(32,), dtype=tf.string, name=None), 'rank': TensorSpec(shape=(32,), dtype=tf.float64, name=None)}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec={'features': TensorSpec(shape=(), dtype=tf.string, name=None), 'encoder': TensorSpec(shape=(32,), dtype=tf.string, name=None), 'rank': TensorSpec(shape=(32,), dtype=tf.float64, name=None)}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listwise_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder': <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'OE', b'CV5TE', b'OHE', b'BUCV5TE', b'BUCV5RGLMME', b'TE',\n",
      "       b'BUCV10TE', b'CV2RGLMME', b'PBTE01', b'BE', b'CV10TE', b'MHE',\n",
      "       b'CV5RGLMME', b'ME10E', b'DTEM2', b'RGLMME', b'BUCV2RGLMME', b'CE',\n",
      "       b'WOEE', b'DE', b'DTEM5', b'CV2TE', b'BUCV2TE', b'ME1E', b'CBE',\n",
      "       b'SE', b'PBTE0001', b'DTEM10', b'BUCV10RGLMME', b'ME01E',\n",
      "       b'CV10RGLMME', b'PBTE001'], dtype=object)>,\n",
      " 'features': <tf.Tensor: shape=(), dtype=string, numpy=b'56 LGBMC no F1'>,\n",
      " 'rank': <tf.Tensor: shape=(32,), dtype=float64, numpy=\n",
      "array([2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0.,\n",
      "       0., 2., 3., 0., 0., 0., 2., 1., 2., 0., 0., 0., 2., 0., 0.])>}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for example in df_listwise.take(1):\n",
    "  pprint.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = df_listwise.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = df_listwise_test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset element_spec={'features': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'encoder': TensorSpec(shape=(None, 32), dtype=tf.string, name=None), 'rank': TensorSpec(shape=(None, 32), dtype=tf.float64, name=None)}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a array containing all unique combinations of model, tuning, scoring as byte strings\n",
    "# unique_factor_combinations = np.unique(df_listwise[['model', 'tuning', 'scoring']])\n",
    "# unique_factor_combinations = unique_factor_combinations.astype('S')\n",
    "# print(unique_factor_combinations)\n",
    "\n",
    "# unique_model_combinations = np.unique(df_listwise['model'])\n",
    "# unique_model_combinations = unique_factor_combinations.astype('S')\n",
    "\n",
    "# unique_tuning_combinations = np.unique(df_listwise['tuning'])\n",
    "# unique_tuning_combinations = unique_factor_combinations.astype('S')\n",
    "\n",
    "unique_factor_combinations = np.unique(df[['features']])\n",
    "unique_factor_combinations = unique_factor_combinations.astype('S')\n",
    "\n",
    "unique_encoder_rankings = np.unique(df[['encoder']])\n",
    "unique_encoder_rankings = unique_encoder_rankings.astype('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss(), unique_factor_combinations, unique_encoder_rankings)\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 4s 347ms/step - ndcg_metric: 0.4914 - root_mean_squared_error: 12.9782 - loss: 81.4394 - regularization_loss: 0.0000e+00 - total_loss: 81.4394\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 340ms/step - ndcg_metric: 0.5901 - root_mean_squared_error: 12.8358 - loss: 80.8423 - regularization_loss: 0.0000e+00 - total_loss: 80.8423\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 320ms/step - ndcg_metric: 0.6121 - root_mean_squared_error: 12.8051 - loss: 80.7198 - regularization_loss: 0.0000e+00 - total_loss: 80.7198\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 321ms/step - ndcg_metric: 0.6122 - root_mean_squared_error: 12.7971 - loss: 80.7088 - regularization_loss: 0.0000e+00 - total_loss: 80.7088\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 337ms/step - ndcg_metric: 0.6125 - root_mean_squared_error: 12.7946 - loss: 80.7014 - regularization_loss: 0.0000e+00 - total_loss: 80.7014\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 375ms/step - ndcg_metric: 0.6113 - root_mean_squared_error: 12.7767 - loss: 80.7544 - regularization_loss: 0.0000e+00 - total_loss: 80.7544\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 369ms/step - ndcg_metric: 0.6113 - root_mean_squared_error: 12.7403 - loss: 80.6937 - regularization_loss: 0.0000e+00 - total_loss: 80.6937\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 341ms/step - ndcg_metric: 0.6120 - root_mean_squared_error: 12.7778 - loss: 80.7048 - regularization_loss: 0.0000e+00 - total_loss: 80.7048\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 323ms/step - ndcg_metric: 0.6194 - root_mean_squared_error: 12.7614 - loss: 80.7087 - regularization_loss: 0.0000e+00 - total_loss: 80.7087\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 321ms/step - ndcg_metric: 0.6157 - root_mean_squared_error: 12.7328 - loss: 80.6778 - regularization_loss: 0.0000e+00 - total_loss: 80.6778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a7b51434d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model.fit(cached_train, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: listwise_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: listwise_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the listwise model\n",
    "listwise_model.save('listwise_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model('listwise_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1850, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (1 total):\n        * {'encoder': <tf.Tensor 'features_1:0' shape=(None, 32) dtype=string>,\n     'features': <tf.Tensor 'features:0' shape=(None,) dtype=string>,\n     'rank': <tf.Tensor 'features_2:0' shape=(None, 32) dtype=float32>}\n      Keyword arguments: {'training': False}\n    \n     Expected these arguments to match one of the following 2 option(s):\n    \n    Option 1:\n      Positional arguments (1 total):\n        * {'encoder': TensorSpec(shape=(None, 32), dtype=tf.string, name='encoder'),\n     'features': TensorSpec(shape=(None,), dtype=tf.string, name='features_features')}\n      Keyword arguments: {'training': True}\n    \n    Option 2:\n      Positional arguments (1 total):\n        * {'encoder': TensorSpec(shape=(None, 32), dtype=tf.string, name='encoder'),\n     'features': TensorSpec(shape=(None,), dtype=tf.string, name='features_features')}\n      Keyword arguments: {'training': False}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m listwise_model_result \u001b[39m=\u001b[39m reloaded\u001b[39m.\u001b[39;49mevaluate(cached_test, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNDCG of the MSE Model: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(listwise_model_result[\u001b[39m\"\u001b[39m\u001b[39mndcg_metric\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexgnd6xcc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1850, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (1 total):\n        * {'encoder': <tf.Tensor 'features_1:0' shape=(None, 32) dtype=string>,\n     'features': <tf.Tensor 'features:0' shape=(None,) dtype=string>,\n     'rank': <tf.Tensor 'features_2:0' shape=(None, 32) dtype=float32>}\n      Keyword arguments: {'training': False}\n    \n     Expected these arguments to match one of the following 2 option(s):\n    \n    Option 1:\n      Positional arguments (1 total):\n        * {'encoder': TensorSpec(shape=(None, 32), dtype=tf.string, name='encoder'),\n     'features': TensorSpec(shape=(None,), dtype=tf.string, name='features_features')}\n      Keyword arguments: {'training': True}\n    \n    Option 2:\n      Positional arguments (1 total):\n        * {'encoder': TensorSpec(shape=(None, 32), dtype=tf.string, name='encoder'),\n     'features': TensorSpec(shape=(None,), dtype=tf.string, name='features_features')}\n      Keyword arguments: {'training': False}\n"
     ]
    }
   ],
   "source": [
    "listwise_model_result = reloaded.evaluate(cached_test, return_dict=True)\n",
    "print(\"NDCG of the MSE Model: {:.4f}\".format(listwise_model_result[\"ndcg_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 79ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7552, 32, 1)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = listwise_model.predict(cached_test)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.69773453],\n",
       "        [ 0.5882609 ],\n",
       "        [ 0.56097853],\n",
       "        ...,\n",
       "        [ 0.82760644],\n",
       "        [ 0.4474638 ],\n",
       "        [ 1.0411758 ]],\n",
       "\n",
       "       [[ 0.7542254 ],\n",
       "        [ 1.0411758 ],\n",
       "        [ 0.619869  ],\n",
       "        ...,\n",
       "        [ 0.841596  ],\n",
       "        [ 0.663063  ],\n",
       "        [ 0.69312024]],\n",
       "\n",
       "       [[ 0.045226  ],\n",
       "        [ 0.7616128 ],\n",
       "        [ 0.06197588],\n",
       "        ...,\n",
       "        [ 0.9450989 ],\n",
       "        [ 0.663063  ],\n",
       "        [ 0.39793494]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.62416464],\n",
       "        [ 0.619869  ],\n",
       "        [ 0.4474638 ],\n",
       "        ...,\n",
       "        [-0.01573833],\n",
       "        [ 2.6315248 ],\n",
       "        [ 0.82760644]],\n",
       "\n",
       "       [[ 1.0411758 ],\n",
       "        [ 1.0975684 ],\n",
       "        [ 0.15994318],\n",
       "        ...,\n",
       "        [ 0.4474638 ],\n",
       "        [ 0.7127183 ],\n",
       "        [ 0.62416464]],\n",
       "\n",
       "       [[ 0.663063  ],\n",
       "        [-0.01573833],\n",
       "        [ 0.63231915],\n",
       "        ...,\n",
       "        [ 0.15994318],\n",
       "        [ 0.69312024],\n",
       "        [ 0.75579476]]], dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
