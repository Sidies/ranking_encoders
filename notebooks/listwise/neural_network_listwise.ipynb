{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import configuration as config\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset     object\n",
      "model       object\n",
      "tuning      object\n",
      "scoring     object\n",
      "encoder     object\n",
      "rank       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df = config.load_traindata_for_pointwise()\n",
    "df = df.drop(columns=['cv_score'])\n",
    "df['dataset'] = df['dataset'].astype(str)\n",
    "print(df.dtypes)\n",
    "df.head()\n",
    "features = ['dataset', 'model', 'tuning', 'scoring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "df_tf = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "print(type(df_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import collections\n",
    "\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "\n",
    "def _create_feature_dict() -> Dict[Text, List[tf.Tensor]]:\n",
    "  \"\"\"Helper function for creating an empty feature dict for defaultdict.\"\"\"\n",
    "  return {\"encoder\": [], \"rank\": []}\n",
    "\n",
    "\n",
    "def _sample_list(\n",
    "    feature_lists: Dict[Text, List[tf.Tensor]],\n",
    "    num_examples_per_list: int,\n",
    "    random_state: Optional[np.random.RandomState] = None,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Function for sampling a list example from given feature lists.\"\"\"\n",
    "  if random_state is None:\n",
    "    random_state = np.random.RandomState()\n",
    "\n",
    "  sampled_indices = random_state.choice(\n",
    "      range(len(feature_lists[\"encoder\"])),\n",
    "      size=num_examples_per_list,\n",
    "      replace=False,\n",
    "  )\n",
    "  sampled_movie_titles = [\n",
    "      feature_lists[\"encoder\"][idx] for idx in sampled_indices\n",
    "  ]\n",
    "  sampled_ratings = [\n",
    "      feature_lists[\"rank\"][idx]\n",
    "      for idx in sampled_indices\n",
    "  ]\n",
    "\n",
    "  return (\n",
    "      tf.stack(sampled_movie_titles, 0),\n",
    "      tf.stack(sampled_ratings, 0),\n",
    "  )\n",
    "\n",
    "\n",
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    num_list_per_user: int = 10,\n",
    "    num_examples_per_list: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "  \"\"\"Function for converting the MovieLens 100K dataset to a listwise dataset.\n",
    "\n",
    "  Args:\n",
    "      rating_dataset:\n",
    "        The MovieLens ratings dataset loaded from TFDS with features\n",
    "        \"movie_title\", \"user_id\", and \"user_rating\".\n",
    "      num_list_per_user:\n",
    "        An integer representing the number of lists that should be sampled for\n",
    "        each user in the training dataset.\n",
    "      num_examples_per_list:\n",
    "        An integer representing the number of movies to be sampled for each list\n",
    "        from the list of movies rated by the user.\n",
    "      seed:\n",
    "        An integer for creating `np.random.RandomState`.\n",
    "\n",
    "  Returns:\n",
    "      A tf.data.Dataset containing list examples.\n",
    "\n",
    "      Each example contains three keys: \"user_id\", \"movie_title\", and\n",
    "      \"user_rating\". \"user_id\" maps to a string tensor that represents the user\n",
    "      id for the example. \"movie_title\" maps to a tensor of shape\n",
    "      [sum(num_example_per_list)] with dtype tf.string. It represents the list\n",
    "      of candidate movie ids. \"user_rating\" maps to a tensor of shape\n",
    "      [sum(num_example_per_list)] with dtype tf.float32. It represents the\n",
    "      rating of each movie in the candidate list.\n",
    "  \"\"\"\n",
    "  random_state = np.random.RandomState(seed)\n",
    "\n",
    "  example_lists_by_user = collections.defaultdict(_create_feature_dict)\n",
    "\n",
    "  movie_title_vocab = set()\n",
    "  for example in rating_dataset:\n",
    "    user_id = example[\"dataset\"].numpy()\n",
    "    example_lists_by_user[user_id][\"encoder\"].append(\n",
    "        example[\"encoder\"])\n",
    "    example_lists_by_user[user_id][\"rank\"].append(\n",
    "        example[\"rank\"])\n",
    "    movie_title_vocab.add(example[\"encoder\"].numpy())\n",
    "\n",
    "  tensor_slices = {\"dataset\": [], \"encoder\": [], \"rank\": []}\n",
    "\n",
    "  for user_id, feature_lists in example_lists_by_user.items():\n",
    "    for _ in range(num_list_per_user):\n",
    "\n",
    "      # Drop the user if they don't have enough ratings.\n",
    "      if len(feature_lists[\"encoder\"]) < num_examples_per_list:\n",
    "        continue\n",
    "\n",
    "      sampled_movie_titles, sampled_ratings = _sample_list(\n",
    "          feature_lists,\n",
    "          num_examples_per_list,\n",
    "          random_state=random_state,\n",
    "      )\n",
    "      tensor_slices[\"dataset\"].append(user_id)\n",
    "      tensor_slices[\"encoder\"].append(sampled_movie_titles)\n",
    "      tensor_slices[\"rank\"].append(sampled_ratings)\n",
    "\n",
    "  return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "df_listwise = sample_listwise(df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec={'dataset': TensorSpec(shape=(), dtype=tf.string, name=None), 'encoder': TensorSpec(shape=(10,), dtype=tf.string, name=None), 'rank': TensorSpec(shape=(10,), dtype=tf.float64, name=None)}>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': <tf.Tensor: shape=(), dtype=string, numpy=b'1169'>,\n",
      " 'encoder': <tf.Tensor: shape=(10,), dtype=string, numpy=\n",
      "array([b'CV2RGLMME', b'CV10RGLMME', b'CV5RGLMME', b'DTEM10', b'CV2TE',\n",
      "       b'CV10RGLMME', b'CV2RGLMME', b'BE', b'CBE', b'OE'], dtype=object)>,\n",
      " 'rank': <tf.Tensor: shape=(10,), dtype=float64, numpy=array([15., 23., 17., 24.,  6.,  7., 18., 15.,  0., 25.])>}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for example in df_listwise.take(1):\n",
    "  pprint.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "cached_train = df_listwise.shuffle(100_000).batch(8192).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset element_spec={'dataset': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'encoder': TensorSpec(shape=(None, 10), dtype=tf.string, name=None), 'rank': TensorSpec(shape=(None, 10), dtype=tf.float64, name=None)}>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'1037' b'1111' b'1112' b'1114' b'1169' b'1235' b'1461' b'1463' b'1486'\n",
      " b'1506' b'1511' b'1590' b'23381' b'29' b'3' b'31' b'333' b'334' b'38'\n",
      " b'40536' b'40945' b'40981' b'40999' b'41005' b'41007' b'41162' b'41224'\n",
      " b'42178' b'42343' b'42344' b'42738' b'42750' b'43098' b'43607' b'43890'\n",
      " b'43892' b'43896' b'43897' b'43900' b'43922' b'451' b'470' b'50' b'51'\n",
      " b'56' b'6332' b'881' b'956' b'959' b'981']\n",
      "[b'BE' b'BUCV10RGLMME' b'BUCV10TE' b'BUCV2RGLMME' b'BUCV2TE'\n",
      " b'BUCV5RGLMME' b'BUCV5TE' b'CBE' b'CE' b'CV10RGLMME' b'CV10TE'\n",
      " b'CV2RGLMME' b'CV2TE' b'CV5RGLMME' b'CV5TE' b'DE' b'DTEM10' b'DTEM2'\n",
      " b'DTEM5' b'ME01E' b'ME10E' b'ME1E' b'MHE' b'OE' b'OHE' b'PBTE0001'\n",
      " b'PBTE001' b'PBTE01' b'RGLMME' b'SE' b'TE' b'WOEE']\n"
     ]
    }
   ],
   "source": [
    "# convert to a array containing all unique combinations of model, tuning, scoring as byte strings\n",
    "# unique_factor_combinations = np.unique(df_listwise[['model', 'tuning', 'scoring']])\n",
    "# unique_factor_combinations = unique_factor_combinations.astype('S')\n",
    "# print(unique_factor_combinations)\n",
    "\n",
    "# unique_model_combinations = np.unique(df_listwise['model'])\n",
    "# unique_model_combinations = unique_factor_combinations.astype('S')\n",
    "\n",
    "# unique_tuning_combinations = np.unique(df_listwise['tuning'])\n",
    "# unique_tuning_combinations = unique_factor_combinations.astype('S')\n",
    "\n",
    "unique_factor_combinations = np.unique(df[['dataset']])\n",
    "unique_factor_combinations = unique_factor_combinations.astype('S')\n",
    "print(unique_factor_combinations)\n",
    "\n",
    "unique_encoder_rankings = np.unique(df[['encoder']])\n",
    "unique_encoder_rankings = unique_encoder_rankings.astype('S')\n",
    "print(unique_encoder_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)\n",
    "\n",
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "    print(\"STARTING INIT\")\n",
    "    # Compute embeddings for factor combinations.\n",
    "    self.factors_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_factor_combinations),\n",
    "      tf.keras.layers.Embedding(len(unique_factor_combinations) + 2, embedding_dimension)\n",
    "    ])\n",
    "    \n",
    "    # Compute embeddings for encoder combinations.\n",
    "    self.encoder_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_encoder_rankings),\n",
    "      tf.keras.layers.Embedding(len(unique_encoder_rankings) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "    print(\"FINISHED INIT\")\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.factors_embeddings(features[\"dataset\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.encoder_embeddings(features[\"encoder\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    print(features[\"encoder\"].shape)\n",
    "    print(features[\"dataset\"].shape)\n",
    "    #list_length = features[\"encoder\"].shape[1]\n",
    "    # get list length for my shape (10,) tensor\n",
    "    list_length = features[\"encoder\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"rank\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING INIT\n",
      "FINISHED INIT\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "(None, 10)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(None,)\n",
      "(None, 10)\n",
      "(None,)\n",
      "1/1 [==============================] - 4s 4s/step - ndcg_metric: 0.4895 - root_mean_squared_error: 13.1398 - loss: 15.1032 - regularization_loss: 0.0000e+00 - total_loss: 15.1032\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 28ms/step - ndcg_metric: 0.4955 - root_mean_squared_error: 13.1354 - loss: 15.0953 - regularization_loss: 0.0000e+00 - total_loss: 15.0953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bdd89232d0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model.fit(cached_train, epochs=2, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
