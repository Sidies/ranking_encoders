{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import configuration as config\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset      int64\n",
      "model       object\n",
      "tuning      object\n",
      "scoring     object\n",
      "encoder     object\n",
      "rank       float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>tuning</th>\n",
       "      <th>scoring</th>\n",
       "      <th>encoder</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>BUCV2RGLMME</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>BUCV2TE</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CBE</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CE</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CV10RGLMME</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset model tuning scoring      encoder  rank\n",
       "0     1169   KNC  model     ACC  BUCV2RGLMME  16.0\n",
       "1     1169   KNC  model     ACC      BUCV2TE  14.0\n",
       "2     1169   KNC  model     ACC          CBE  22.0\n",
       "3     1169   KNC  model     ACC           CE  23.0\n",
       "4     1169   KNC  model     ACC   CV10RGLMME   7.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = config.load_traindata_for_pointwise()\n",
    "df = df.drop(columns=['cv_score'])\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset model tuning scoring                               encoder_rankings  \\\n",
      "0    1169   KNC  model     ACC  [[CE, CBE, BUCV2RGLMME, BUCV2TE, CV10RGLMME]]   \n",
      "\n",
      "                           ranking  \n",
      "0  [[23.0, 22.0, 16.0, 14.0, 7.0]]  \n"
     ]
    }
   ],
   "source": [
    "def create_encoder_rankings(df):\n",
    "    # Group the DataFrame by 'dataset', 'model', 'tuning', and 'scoring' columns\n",
    "    grouped_df = df.groupby(['dataset', 'model', 'tuning', 'scoring'])\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    new_df = pd.DataFrame(columns=['dataset', 'model', 'tuning', 'scoring', 'encoder_rankings'])\n",
    "    \n",
    "    for group_keys, group_data in grouped_df:\n",
    "        dataset, model, tuning, scoring = group_keys\n",
    "        encoder_rankings = group_data.sort_values('rank', ascending=False)['encoder'].tolist()\n",
    "        rankings = group_data.sort_values('rank', ascending=False)['rank'].tolist()\n",
    "        new_row = {'dataset': dataset, 'model': model, 'tuning': tuning, 'scoring': scoring,\n",
    "                   'encoder_rankings': [encoder_rankings], 'ranking': [rankings]}\n",
    "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Your original DataFrame\n",
    "data = {\n",
    "    'dataset': [1169, 1169, 1169, 1169, 1169],\n",
    "    'model': ['KNC', 'KNC', 'KNC', 'KNC', 'KNC'],\n",
    "    'tuning': ['model', 'model', 'model', 'model', 'model'],\n",
    "    'scoring': ['ACC', 'ACC', 'ACC', 'ACC', 'ACC'],\n",
    "    'encoder': ['BUCV2RGLMME', 'BUCV2TE', 'CBE', 'CE', 'CV10RGLMME'],\n",
    "    'rank': [16.0, 14.0, 22.0, 23.0, 7.0]\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "# Call the method to create the new DataFrame\n",
    "new_dataframe = create_encoder_rankings(test_df)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(new_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset model tuning scoring  \\\n",
      "0       3   DTC   full     ACC   \n",
      "1       3   DTC   full     AUC   \n",
      "2       3   DTC   full      F1   \n",
      "3       3   DTC  model     AUC   \n",
      "4       3   DTC  model      F1   \n",
      "\n",
      "                                    encoder_rankings  \\\n",
      "0  [[DE, CBE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...   \n",
      "1  [[DE, CBE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...   \n",
      "2  [[CBE, DE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...   \n",
      "3  [[DE, CBE, PBTE01, CV2TE, CV2RGLMME, CV5RGLMME...   \n",
      "4  [[CBE, DE, PBTE01, CV2TE, CV2RGLMME, CV5RGLMME...   \n",
      "\n",
      "                                             ranking  \n",
      "0  [[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
      "1  [[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
      "2  [[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
      "3  [[25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18...  \n",
      "4  [[25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18...  \n"
     ]
    }
   ],
   "source": [
    "df_listwise = create_encoder_rankings(df)\n",
    "print(df_listwise.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'dataset')>, 'model': <KerasTensor: shape=(None,) dtype=string (created by layer 'model')>, 'tuning': <KerasTensor: shape=(None,) dtype=string (created by layer 'tuning')>, 'scoring': <KerasTensor: shape=(None,) dtype=string (created by layer 'scoring')>, 'encoder_rankings': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'encoder_rankings')>}\n"
     ]
    }
   ],
   "source": [
    "numerical_feature_names = ['dataset']\n",
    "categorical_feature_names = ['model', 'tuning', 'scoring', 'encoder_rankings']\n",
    "target = df.pop('rank')\n",
    "\n",
    "df = df_listwise\n",
    "\n",
    "inputs = {}\n",
    "for name, column in df.items():\n",
    "  if type(column[0]) == str:\n",
    "    dtype = tf.string\n",
    "  elif (name in categorical_feature_names or\n",
    "        name in numerical_feature_names):\n",
    "    dtype = tf.int64\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "  \n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m numeric_features \u001b[39m=\u001b[39m df[numerical_feature_names]\n\u001b[0;32m      8\u001b[0m normalizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mNormalization(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m normalizer\u001b[39m.\u001b[39madapt(stack_dict(\u001b[39mdict\u001b[39;49m(numeric_features)))\n\u001b[0;32m     11\u001b[0m numeric_inputs \u001b[39m=\u001b[39m {}\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m numerical_feature_names:\n",
      "Cell \u001b[1;32mIn[82], line 4\u001b[0m, in \u001b[0;36mstack_dict\u001b[1;34m(inputs, fun)\u001b[0m\n\u001b[0;32m      2\u001b[0m values \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(inputs\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m----> 4\u001b[0m   values\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39;49mcast(inputs[key], tf\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m fun(values, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "preprocessed = []\n",
    "\n",
    "# preprocessed.append(inputs['dataset'])\n",
    "# preprocessed.append(inputs['rank'])\n",
    "\n",
    "numeric_features = df[numerical_feature_names]\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(stack_dict(dict(numeric_features)))\n",
    "\n",
    "numeric_inputs = {}\n",
    "for name in numerical_feature_names:\n",
    "  numeric_inputs[name]=inputs[name]\n",
    "\n",
    "numeric_inputs = stack_dict(numeric_inputs)\n",
    "numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "preprocessed.append(numeric_normalized)\n",
    "\n",
    "for name in categorical_feature_names:\n",
    "  vocab = sorted(set(df[name]))\n",
    "  print(f'name: {name}')\n",
    "  print(f'vocab: {vocab}\\n')\n",
    "\n",
    "  if type(vocab[0]) is str:\n",
    "    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "  else:\n",
    "    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "  x = inputs[name][:, tf.newaxis]\n",
    "  x = lookup(x)\n",
    "  preprocessed.append(x)\n",
    "  \n",
    "  print(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 48) dtype=float32 (created by layer 'tf.concat_3')>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesssed_result = tf.concat(preprocessed, axis=-1)\n",
    "preprocesssed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = tf.keras.Model(inputs, preprocesssed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 48) dtype=float32 (created by layer 'model_6')>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preprocessor(inputs)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'sequential_3')>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = body(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs, result)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18027/18027 [==============================] - 45s 2ms/step - loss: -2824638.2500 - accuracy: 0.0695\n",
      "Epoch 2/5\n",
      "18027/18027 [==============================] - 45s 2ms/step - loss: -30646428.0000 - accuracy: 0.0694\n",
      "Epoch 3/5\n",
      "18027/18027 [==============================] - 44s 2ms/step - loss: -114108120.0000 - accuracy: 0.0694\n",
      "Epoch 4/5\n",
      "18027/18027 [==============================] - 46s 3ms/step - loss: -282837984.0000 - accuracy: 0.0694\n",
      "Epoch 5/5\n",
      "18027/18027 [==============================] - 54s 3ms/step - loss: -566133504.0000 - accuracy: 0.0694\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dict(df), target, epochs=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Marco\\tensorflow_datasets\\movielens\\100k-movies\\0.1.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bea18d5d1a4bef9d94a779d1ea268a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d39057a0ea4934ae73ec3b9fdfc2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8971b78372d4102bc892e8d77425952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65af39fea9043a296b1163674355edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896d0d6237ab4c149dda218ed524a683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0989dd7b874f46af58a30da2e13a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Marco\\tensorflow_datasets\\movielens\\100k-movies\\0.1.1.incomplete84VJJ8\\movielens-train.tfre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to C:\\Users\\Marco\\tensorflow_datasets\\movielens\\100k-movies\\0.1.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Split between train and tests sets, as before.\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "test = tfrs.examples.movielens.sample_listwise(\n",
    "    test,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'Postman, The (1997)', b'Liar Liar (1997)', b'Contact (1997)',\n",
      "       b'Welcome To Sarajevo (1997)',\n",
      "       b'I Know What You Did Last Summer (1997)'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'681'>,\n",
      " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 5., 1., 4., 1.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for example in train.take(1):\n",
    "  pprint.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_titles),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6/6 [==============================] - 3s 133ms/step - ndcg_metric: 0.8545 - root_mean_squared_error: 3.7307 - loss: 4.7866 - regularization_loss: 0.0000e+00 - total_loss: 4.7866\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 1s 129ms/step - ndcg_metric: 0.8644 - root_mean_squared_error: 3.7196 - loss: 4.7845 - regularization_loss: 0.0000e+00 - total_loss: 4.7845\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 1s 145ms/step - ndcg_metric: 0.8699 - root_mean_squared_error: 3.7112 - loss: 4.7821 - regularization_loss: 0.0000e+00 - total_loss: 4.7821\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 1s 122ms/step - ndcg_metric: 0.8737 - root_mean_squared_error: 3.7067 - loss: 4.7791 - regularization_loss: 0.0000e+00 - total_loss: 4.7791\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.8770 - root_mean_squared_error: 3.6981 - loss: 4.7748 - regularization_loss: 0.0000e+00 - total_loss: 4.7748\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.8803 - root_mean_squared_error: 3.6903 - loss: 4.7689 - regularization_loss: 0.0000e+00 - total_loss: 4.7689\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.8837 - root_mean_squared_error: 3.6814 - loss: 4.7596 - regularization_loss: 0.0000e+00 - total_loss: 4.7596\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.8873 - root_mean_squared_error: 3.6716 - loss: 4.7468 - regularization_loss: 0.0000e+00 - total_loss: 4.7468\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.8907 - root_mean_squared_error: 3.6568 - loss: 4.7299 - regularization_loss: 0.0000e+00 - total_loss: 4.7299\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.8940 - root_mean_squared_error: 3.6332 - loss: 4.7067 - regularization_loss: 0.0000e+00 - total_loss: 4.7067\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 1s 129ms/step - ndcg_metric: 0.8968 - root_mean_squared_error: 3.5966 - loss: 4.6784 - regularization_loss: 0.0000e+00 - total_loss: 4.6784\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 1s 116ms/step - ndcg_metric: 0.8995 - root_mean_squared_error: 3.5458 - loss: 4.6491 - regularization_loss: 0.0000e+00 - total_loss: 4.6491\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 1s 119ms/step - ndcg_metric: 0.9020 - root_mean_squared_error: 3.4750 - loss: 4.6231 - regularization_loss: 0.0000e+00 - total_loss: 4.6231\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9040 - root_mean_squared_error: 3.3964 - loss: 4.5990 - regularization_loss: 0.0000e+00 - total_loss: 4.5990\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 1s 121ms/step - ndcg_metric: 0.9057 - root_mean_squared_error: 3.3274 - loss: 4.5787 - regularization_loss: 0.0000e+00 - total_loss: 4.5787\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 1s 115ms/step - ndcg_metric: 0.9071 - root_mean_squared_error: 3.2640 - loss: 4.5650 - regularization_loss: 0.0000e+00 - total_loss: 4.5650\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 1s 119ms/step - ndcg_metric: 0.9082 - root_mean_squared_error: 3.2130 - loss: 4.5515 - regularization_loss: 0.0000e+00 - total_loss: 4.5515\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9090 - root_mean_squared_error: 3.1600 - loss: 4.5384 - regularization_loss: 0.0000e+00 - total_loss: 4.5384\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.9098 - root_mean_squared_error: 3.1097 - loss: 4.5321 - regularization_loss: 0.0000e+00 - total_loss: 4.5321\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 1s 119ms/step - ndcg_metric: 0.9104 - root_mean_squared_error: 3.0647 - loss: 4.5231 - regularization_loss: 0.0000e+00 - total_loss: 4.5231\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9109 - root_mean_squared_error: 3.0426 - loss: 4.5154 - regularization_loss: 0.0000e+00 - total_loss: 4.5154\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 1s 115ms/step - ndcg_metric: 0.9113 - root_mean_squared_error: 3.0117 - loss: 4.5121 - regularization_loss: 0.0000e+00 - total_loss: 4.5121\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9116 - root_mean_squared_error: 2.9923 - loss: 4.5071 - regularization_loss: 0.0000e+00 - total_loss: 4.5071\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9118 - root_mean_squared_error: 2.9746 - loss: 4.5028 - regularization_loss: 0.0000e+00 - total_loss: 4.5028\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 1s 116ms/step - ndcg_metric: 0.9121 - root_mean_squared_error: 2.9584 - loss: 4.5013 - regularization_loss: 0.0000e+00 - total_loss: 4.5013\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 1s 120ms/step - ndcg_metric: 0.9124 - root_mean_squared_error: 2.9377 - loss: 4.5015 - regularization_loss: 0.0000e+00 - total_loss: 4.5015\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 1s 124ms/step - ndcg_metric: 0.9125 - root_mean_squared_error: 2.9273 - loss: 4.4961 - regularization_loss: 0.0000e+00 - total_loss: 4.4961\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 1s 130ms/step - ndcg_metric: 0.9127 - root_mean_squared_error: 2.9147 - loss: 4.4948 - regularization_loss: 0.0000e+00 - total_loss: 4.4948\n",
      "Epoch 29/30\n",
      "3/6 [==============>...............] - ETA: 0s - ndcg_metric: 0.9126 - root_mean_squared_error: 2.9219 - loss: 4.4938 - regularization_loss: 0.0000e+00 - total_loss: 4.4938"
     ]
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(cached_train, epochs=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot iterate over a Tensor with unknown first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 102\u001b[0m\n\u001b[0;32m     98\u001b[0m       tensor_slices[\u001b[39m\"\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(sampled_ratings)\n\u001b[0;32m    100\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(tensor_slices)\n\u001b[1;32m--> 102\u001b[0m test \u001b[39m=\u001b[39m sample_listwise(x, \u001b[39m1\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m42\u001b[39;49m)\n\u001b[0;32m    103\u001b[0m \u001b[39mprint\u001b[39m(test)\n",
      "Cell \u001b[1;32mIn[67], line 74\u001b[0m, in \u001b[0;36msample_listwise\u001b[1;34m(rating_dataset, num_list_per_user, num_examples_per_list, seed)\u001b[0m\n\u001b[0;32m     71\u001b[0m example_lists_by_user \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mdefaultdict(_create_feature_dict)\n\u001b[0;32m     73\u001b[0m movie_title_vocab \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m---> 74\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m rating_dataset:\n\u001b[0;32m     75\u001b[0m   user_id \u001b[39m=\u001b[39m example[\u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     76\u001b[0m   example_lists_by_user[user_id][\u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     77\u001b[0m       example[\u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\keras_tensor.py:410\u001b[0m, in \u001b[0;36mKerasTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a scalar.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a Tensor with unknown first dimension.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m \u001b[39mreturn\u001b[39;00m _KerasTensorIterator(\u001b[39mself\u001b[39m, shape[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."
     ]
    }
   ],
   "source": [
    "\n",
    "import array\n",
    "import collections\n",
    "\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "\n",
    "def _create_feature_dict() -> Dict[Text, List[tf.Tensor]]:\n",
    "  \"\"\"Helper function for creating an empty feature dict for defaultdict.\"\"\"\n",
    "  return {\"encoder\": [], \"rank\": []}\n",
    "\n",
    "def _sample_list(\n",
    "    feature_lists: Dict[Text, List[tf.Tensor]],\n",
    "    num_examples_per_list: int,\n",
    "    random_state: Optional[np.random.RandomState] = None,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Function for sampling a list example from given feature lists.\"\"\"\n",
    "  if random_state is None:\n",
    "    random_state = np.random.RandomState()\n",
    "\n",
    "  sampled_indices = random_state.choice(\n",
    "      range(len(feature_lists[\"encoder\"])),\n",
    "      size=num_examples_per_list,\n",
    "      replace=False,\n",
    "  )\n",
    "  sampled_movie_titles = [\n",
    "      feature_lists[\"encoder\"][idx] for idx in sampled_indices\n",
    "  ]\n",
    "  sampled_ratings = [\n",
    "      feature_lists[\"rank\"][idx]\n",
    "      for idx in sampled_indices\n",
    "  ]\n",
    "\n",
    "  return (\n",
    "      tf.stack(sampled_movie_titles, 0),\n",
    "      tf.stack(sampled_ratings, 0),\n",
    "  )\n",
    "\n",
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    num_list_per_user: int = 10,\n",
    "    num_examples_per_list: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "  \"\"\"Function for converting the MovieLens 100K dataset to a listwise dataset.\n",
    "\n",
    "  Args:\n",
    "      rating_dataset:\n",
    "        The MovieLens ratings dataset loaded from TFDS with features\n",
    "        \"movie_title\", \"user_id\", and \"user_rating\".\n",
    "      num_list_per_user:\n",
    "        An integer representing the number of lists that should be sampled for\n",
    "        each user in the training dataset.\n",
    "      num_examples_per_list:\n",
    "        An integer representing the number of movies to be sampled for each list\n",
    "        from the list of movies rated by the user.\n",
    "      seed:\n",
    "        An integer for creating `np.random.RandomState`.\n",
    "\n",
    "  Returns:\n",
    "      A tf.data.Dataset containing list examples.\n",
    "\n",
    "      Each example contains three keys: \"user_id\", \"movie_title\", and\n",
    "      \"user_rating\". \"user_id\" maps to a string tensor that represents the user\n",
    "      id for the example. \"movie_title\" maps to a tensor of shape\n",
    "      [sum(num_example_per_list)] with dtype tf.string. It represents the list\n",
    "      of candidate movie ids. \"user_rating\" maps to a tensor of shape\n",
    "      [sum(num_example_per_list)] with dtype tf.float32. It represents the\n",
    "      rating of each movie in the candidate list.\n",
    "  \"\"\"\n",
    "  random_state = np.random.RandomState(seed)\n",
    "\n",
    "  example_lists_by_user = collections.defaultdict(_create_feature_dict)\n",
    "\n",
    "  movie_title_vocab = set()\n",
    "  for example in rating_dataset:\n",
    "    user_id = example[\"dataset\"].numpy()\n",
    "    example_lists_by_user[user_id][\"encoder\"].append(\n",
    "        example[\"encoder\"])\n",
    "    example_lists_by_user[user_id][\"rank\"].append(\n",
    "        example[\"rank\"])\n",
    "    movie_title_vocab.add(example[\"encoder\"].numpy())\n",
    "\n",
    "  tensor_slices = {\"dataset\": [], \"encoder\": [], \"rank\": []}\n",
    "\n",
    "  for user_id, feature_lists in example_lists_by_user.items():\n",
    "    for _ in range(num_list_per_user):\n",
    "\n",
    "      # Drop the user if they don't have enough ratings.\n",
    "      if len(feature_lists[\"encoder\"]) < num_examples_per_list:\n",
    "        continue\n",
    "\n",
    "      sampled_movie_titles, sampled_ratings = _sample_list(\n",
    "          feature_lists,\n",
    "          num_examples_per_list,\n",
    "          random_state=random_state,\n",
    "      )\n",
    "      tensor_slices[\"dataset\"].append(user_id)\n",
    "      tensor_slices[\"encoder\"].append(sampled_movie_titles)\n",
    "      tensor_slices[\"rank\"].append(sampled_ratings)\n",
    "\n",
    "  return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "test = sample_listwise(x, 1, 5, 42)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
