{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import configuration as config\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset      int64\n",
      "model       object\n",
      "tuning      object\n",
      "scoring     object\n",
      "encoder     object\n",
      "rank       float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>tuning</th>\n",
       "      <th>scoring</th>\n",
       "      <th>encoder</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>BUCV2RGLMME</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>BUCV2TE</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CBE</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CE</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CV10RGLMME</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset model tuning scoring      encoder  rank\n",
       "0     1169   KNC  model     ACC  BUCV2RGLMME  16.0\n",
       "1     1169   KNC  model     ACC      BUCV2TE  14.0\n",
       "2     1169   KNC  model     ACC          CBE  22.0\n",
       "3     1169   KNC  model     ACC           CE  23.0\n",
       "4     1169   KNC  model     ACC   CV10RGLMME   7.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "df = config.load_traindata_for_pointwise()\n",
    "df = df.drop(columns=['cv_score'])\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>tuning</th>\n",
       "      <th>scoring</th>\n",
       "      <th>encoder_rankings</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>[[CE, CBE, BUCV2RGLMME, BUCV2TE, CV10RGLMME]]</td>\n",
       "      <td>[[23.0, 22.0, 16.0, 14.0, 7.0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset model tuning scoring                               encoder_rankings  \\\n",
       "0    1169   KNC  model     ACC  [[CE, CBE, BUCV2RGLMME, BUCV2TE, CV10RGLMME]]   \n",
       "\n",
       "                           ranking  \n",
       "0  [[23.0, 22.0, 16.0, 14.0, 7.0]]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_encoder_rankings(df):\n",
    "    # Group the DataFrame by 'dataset', 'model', 'tuning', and 'scoring' columns\n",
    "    grouped_df = df.groupby(['dataset', 'model', 'tuning', 'scoring'])\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    new_df = pd.DataFrame(columns=['dataset', 'model', 'tuning', 'scoring', 'encoder_rankings'])\n",
    "    \n",
    "    for group_keys, group_data in grouped_df:\n",
    "        dataset, model, tuning, scoring = group_keys\n",
    "        encoder_rankings = group_data.sort_values('rank', ascending=False)['encoder'].tolist()\n",
    "        rankings = group_data.sort_values('rank', ascending=False)['rank'].tolist()\n",
    "        new_row = {'dataset': dataset, 'model': model, 'tuning': tuning, 'scoring': scoring,\n",
    "                   'encoder_rankings': [encoder_rankings], 'ranking': [rankings]}\n",
    "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Your original DataFrame\n",
    "data = {\n",
    "    'dataset': [1169, 1169, 1169, 1169, 1169],\n",
    "    'model': ['KNC', 'KNC', 'KNC', 'KNC', 'KNC'],\n",
    "    'tuning': ['model', 'model', 'model', 'model', 'model'],\n",
    "    'scoring': ['ACC', 'ACC', 'ACC', 'ACC', 'ACC'],\n",
    "    'encoder': ['BUCV2RGLMME', 'BUCV2TE', 'CBE', 'CE', 'CV10RGLMME'],\n",
    "    'rank': [16.0, 14.0, 22.0, 23.0, 7.0]\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "# Call the method to create the new DataFrame\n",
    "new_dataframe = create_encoder_rankings(test_df)\n",
    "\n",
    "# Display the new DataFrame\n",
    "new_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>tuning</th>\n",
       "      <th>scoring</th>\n",
       "      <th>encoder_rankings</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>DTC</td>\n",
       "      <td>full</td>\n",
       "      <td>ACC</td>\n",
       "      <td>[[DE, CBE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...</td>\n",
       "      <td>[[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>DTC</td>\n",
       "      <td>full</td>\n",
       "      <td>AUC</td>\n",
       "      <td>[[DE, CBE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...</td>\n",
       "      <td>[[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>DTC</td>\n",
       "      <td>full</td>\n",
       "      <td>F1</td>\n",
       "      <td>[[CBE, DE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...</td>\n",
       "      <td>[[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DTC</td>\n",
       "      <td>model</td>\n",
       "      <td>AUC</td>\n",
       "      <td>[[DE, CBE, PBTE01, CV2TE, CV2RGLMME, CV5RGLMME...</td>\n",
       "      <td>[[25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>DTC</td>\n",
       "      <td>model</td>\n",
       "      <td>F1</td>\n",
       "      <td>[[CBE, DE, PBTE01, CV2TE, CV2RGLMME, CV5RGLMME...</td>\n",
       "      <td>[[25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset model tuning scoring  \\\n",
       "0       3   DTC   full     ACC   \n",
       "1       3   DTC   full     AUC   \n",
       "2       3   DTC   full      F1   \n",
       "3       3   DTC  model     AUC   \n",
       "4       3   DTC  model      F1   \n",
       "\n",
       "                                    encoder_rankings  \\\n",
       "0  [[DE, CBE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...   \n",
       "1  [[DE, CBE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...   \n",
       "2  [[CBE, DE, PBTE01, BE, OE, ME01E, ME10E, ME1E,...   \n",
       "3  [[DE, CBE, PBTE01, CV2TE, CV2RGLMME, CV5RGLMME...   \n",
       "4  [[CBE, DE, PBTE01, CV2TE, CV2RGLMME, CV5RGLMME...   \n",
       "\n",
       "                                             ranking  \n",
       "0  [[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
       "1  [[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
       "2  [[4.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
       "3  [[25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18...  \n",
       "4  [[25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listwise = create_encoder_rankings(df)\n",
    "df_listwise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>tuning</th>\n",
       "      <th>scoring</th>\n",
       "      <th>encoder</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>BUCV2RGLMME</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>BUCV2TE</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CBE</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CE</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1169</td>\n",
       "      <td>KNC</td>\n",
       "      <td>model</td>\n",
       "      <td>ACC</td>\n",
       "      <td>CV10RGLMME</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset model tuning scoring      encoder  rank\n",
       "0     1169   KNC  model     ACC  BUCV2RGLMME  16.0\n",
       "1     1169   KNC  model     ACC      BUCV2TE  14.0\n",
       "2     1169   KNC  model     ACC          CBE  22.0\n",
       "3     1169   KNC  model     ACC           CE  23.0\n",
       "4     1169   KNC  model     ACC   CV10RGLMME   7.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'dataset')>, 'model': <KerasTensor: shape=(None,) dtype=string (created by layer 'model')>, 'tuning': <KerasTensor: shape=(None,) dtype=string (created by layer 'tuning')>, 'scoring': <KerasTensor: shape=(None,) dtype=string (created by layer 'scoring')>, 'encoder': <KerasTensor: shape=(None,) dtype=string (created by layer 'encoder')>}\n"
     ]
    }
   ],
   "source": [
    "numerical_feature_names = ['dataset']\n",
    "categorical_feature_names = ['model', 'tuning', 'scoring', 'encoder']\n",
    "list_feature_names = ['encoder_rankings']\n",
    "df = df.drop(columns=['rank'])\n",
    "target = df_listwise.pop('ranking')\n",
    "\n",
    "inputs = {}\n",
    "for name, column in df.items():\n",
    "  if type(column[0]) == str:\n",
    "    dtype = tf.string\n",
    "  elif (name in categorical_feature_names or\n",
    "        name in numerical_feature_names):\n",
    "    dtype = tf.int64\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "  \n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: model\n",
      "vocab: ['DTC', 'KNC', 'LGBMC', 'LR', 'SVC']\n",
      "\n",
      "name: tuning\n",
      "vocab: ['full', 'model', 'no']\n",
      "\n",
      "name: scoring\n",
      "vocab: ['ACC', 'AUC', 'F1']\n",
      "\n",
      "name: encoder\n",
      "vocab: ['BE', 'BUCV10RGLMME', 'BUCV10TE', 'BUCV2RGLMME', 'BUCV2TE', 'BUCV5RGLMME', 'BUCV5TE', 'CBE', 'CE', 'CV10RGLMME', 'CV10TE', 'CV2RGLMME', 'CV2TE', 'CV5RGLMME', 'CV5TE', 'DE', 'DTEM10', 'DTEM2', 'DTEM5', 'ME01E', 'ME10E', 'ME1E', 'MHE', 'OE', 'OHE', 'PBTE0001', 'PBTE001', 'PBTE01', 'RGLMME', 'SE', 'TE', 'WOEE']\n",
      "\n",
      "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'normalization_3')>, <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'string_lookup_12')>, <KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'string_lookup_13')>, <KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'string_lookup_14')>, <KerasTensor: shape=(None, 33) dtype=float32 (created by layer 'string_lookup_15')>]\n"
     ]
    }
   ],
   "source": [
    "preprocessed = []\n",
    "\n",
    "# preprocessed.append(inputs['dataset'])\n",
    "# preprocessed.append(inputs['rank'])\n",
    "\n",
    "numeric_features = df[numerical_feature_names]\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(stack_dict(dict(numeric_features)))\n",
    "\n",
    "numeric_inputs = {}\n",
    "for name in numerical_feature_names:\n",
    "  numeric_inputs[name]=inputs[name]\n",
    "\n",
    "numeric_inputs = stack_dict(numeric_inputs)\n",
    "numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "preprocessed.append(numeric_normalized)\n",
    "\n",
    "\n",
    "for name in categorical_feature_names:\n",
    "  vocab = sorted(set(df[name]))\n",
    "  print(f'name: {name}')\n",
    "  print(f'vocab: {vocab}\\n')\n",
    "\n",
    "  if type(vocab[0]) is str:\n",
    "    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "  else:\n",
    "    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "  x = inputs[name][:, tf.newaxis]\n",
    "  x = lookup(x)\n",
    "  preprocessed.append(x)\n",
    "\n",
    "# # Convert list feature to separate categorical features\n",
    "# for list_feature_name in list_feature_names:\n",
    "#   unique_strings = set(np.concatenate(df[list_feature_name].values))\n",
    "#   list_lookup = tf.keras.layers.StringLookup(vocabulary=unique_strings, output_mode='one_hot')\n",
    "\n",
    "#   list_features = df[list_feature_name].apply(lambda x: tf.constant(x))\n",
    "#   list_features = list_lookup(list_features)\n",
    "\n",
    "#   # Append the list categorical features to the preprocessed list\n",
    "#   preprocessed.append(list_features)\n",
    "  \n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 48) dtype=float32 (created by layer 'tf.concat_2')>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesssed_result = tf.concat(preprocessed, axis=-1)\n",
    "preprocesssed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = tf.keras.Model(inputs, preprocesssed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 48) dtype=float32 (created by layer 'model_4')>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preprocessor(inputs)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'sequential_2')>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = body(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs, result)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss=tfr.keras.losses.ListMLELoss(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ranking_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    model_embeddings = self.model_embeddings(features[\"model\"])\n",
    "\n",
    "    tuning_embeddings = self.tuning_embeddings(features[\"tuning\"])\n",
    "\n",
    "    # We want to concatenate model embeddings with tuning emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the model\n",
    "    # embeddings to match the shape of tuning embeddings.\n",
    "    list_length = features[\"tuning\"].shape[1]\n",
    "    model_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(model_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.q\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [model_embedding_repeated, tuning_embeddings], 2)\n",
    "\n",
    "    return self.ranking_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"ranking\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RankingModel.__init__() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m listwise_model \u001b[39m=\u001b[39m RankingModel(inputs, result, tfr\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mListMLELoss())\n\u001b[0;32m      2\u001b[0m listwise_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdagrad(\u001b[39m0.1\u001b[39m))\n\u001b[0;32m      3\u001b[0m listwise_model\u001b[39m.\u001b[39mfit(\u001b[39mdict\u001b[39m(df), epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: RankingModel.__init__() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "listwise_model = RankingModel(inputs, result, tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(dict(df), epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\u001b[39mdict\u001b[39;49m(df), target, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "history = model.fit(dict(df), target, epochs=2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m     28\u001b[0m model \u001b[39m=\u001b[39m MyModel()\n\u001b[1;32m---> 30\u001b[0m model\u001b[39m.\u001b[39;49madapt(inputs)\n\u001b[0;32m     32\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     33\u001b[0m               loss\u001b[39m=\u001b[39mtfr\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mListMLELoss(),\n\u001b[0;32m     34\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     35\u001b[0m               run_eagerly\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[129], line 18\u001b[0m, in \u001b[0;36mMyModel.adapt\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m     16\u001b[0m   \u001b[39m# Stack the inputs and `adapt` the normalization layer.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m   inputs \u001b[39m=\u001b[39m stack_dict(inputs)\n\u001b[1;32m---> 18\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalizer\u001b[39m.\u001b[39;49madapt(inputs)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\normalization.py:287\u001b[0m, in \u001b[0;36mNormalization.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the mean and variance of values in a dataset.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \n\u001b[0;32m    244\u001b[0m \u001b[39m    Calling `adapt()` on a `Normalization` layer is an alternative to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_preprocessing_layer.py:246\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m    245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_state()\n\u001b[1;32m--> 246\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[0;32m    247\u001b[0m     data,\n\u001b[0;32m    248\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    249\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m    250\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    251\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m    252\u001b[0m     distribute\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    253\u001b[0m )\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapt_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_adapt_function()\n\u001b[0;32m    255\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1284\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1286\u001b[0m     x,\n\u001b[0;32m   1287\u001b[0m     y,\n\u001b[0;32m   1288\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1289\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1290\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1291\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1292\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1293\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1294\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1295\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1296\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1297\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1298\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[0;32m   1299\u001b[0m )\n\u001b[0;32m   1301\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:479\u001b[0m, in \u001b[0;36mGenericArrayLikeDataAdapter.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    471\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    472\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mKeras is training/fitting/evaluating on array-like data. Keras \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmay not be optimized for this format, so if your input data \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mload a Dataset instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m     )\n\u001b[1;32m--> 479\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:265\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m (sample_weights, _, _) \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m    260\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    261\u001b[0m )\n\u001b[0;32m    263\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m--> 265\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(\n\u001b[0;32m    266\u001b[0m     \u001b[39mint\u001b[39;49m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mflatten(inputs)\n\u001b[0;32m    267\u001b[0m )\u001b[39m.\u001b[39mpop()\n\u001b[0;32m    268\u001b[0m _check_data_cardinality(inputs)\n\u001b[0;32m    270\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# data.  Defaults to `32` for backwards compatibility.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m (sample_weights, _, _) \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m    260\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    261\u001b[0m )\n\u001b[0;32m    263\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[0;32m    265\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m--> 266\u001b[0m     \u001b[39mint\u001b[39;49m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[0;32m    267\u001b[0m )\u001b[39m.\u001b[39mpop()\n\u001b[0;32m    268\u001b[0m _check_data_cardinality(inputs)\n\u001b[0;32m    270\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# data.  Defaults to `32` for backwards compatibility.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    # Create all the internal layers in init.\n",
    "    super().__init__(self)\n",
    "\n",
    "    self.normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      self.normalizer,\n",
    "      tf.keras.layers.Dense(10, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "  def adapt(self, inputs):\n",
    "    # Stack the inputs and `adapt` the normalization layer.\n",
    "    inputs = stack_dict(inputs)\n",
    "    self.normalizer.adapt(inputs)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Stack the inputs\n",
    "    inputs = stack_dict(inputs)\n",
    "    # Run them through all the layers.\n",
    "    result = self.seq(inputs)\n",
    "\n",
    "    return result\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "model.adapt(inputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tfr.keras.losses.ListMLELoss(),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\u001b[39mdict\u001b[39;49m(numeric_features), target, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "model.fit(dict(numeric_features), target, epochs=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n",
      "<class 'tensorflow.python.data.ops.map_op._MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "print(type(ratings))\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "})\n",
    "print(type(ratings))\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Split between train and tests sets, as before.\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "test = tfrs.examples.movielens.sample_listwise(\n",
    "    test,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'Postman, The (1997)', b'Liar Liar (1997)', b'Contact (1997)',\n",
      "       b'Welcome To Sarajevo (1997)',\n",
      "       b'I Know What You Did Last Summer (1997)'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'681'>,\n",
      " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=array([4., 5., 1., 4., 1.], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for example in train.take(1):\n",
    "  pprint.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_titles),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.q\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6/6 [==============================] - 3s 133ms/step - ndcg_metric: 0.8545 - root_mean_squared_error: 3.7307 - loss: 4.7866 - regularization_loss: 0.0000e+00 - total_loss: 4.7866\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 1s 129ms/step - ndcg_metric: 0.8644 - root_mean_squared_error: 3.7196 - loss: 4.7845 - regularization_loss: 0.0000e+00 - total_loss: 4.7845\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 1s 145ms/step - ndcg_metric: 0.8699 - root_mean_squared_error: 3.7112 - loss: 4.7821 - regularization_loss: 0.0000e+00 - total_loss: 4.7821\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 1s 122ms/step - ndcg_metric: 0.8737 - root_mean_squared_error: 3.7067 - loss: 4.7791 - regularization_loss: 0.0000e+00 - total_loss: 4.7791\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.8770 - root_mean_squared_error: 3.6981 - loss: 4.7748 - regularization_loss: 0.0000e+00 - total_loss: 4.7748\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.8803 - root_mean_squared_error: 3.6903 - loss: 4.7689 - regularization_loss: 0.0000e+00 - total_loss: 4.7689\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.8837 - root_mean_squared_error: 3.6814 - loss: 4.7596 - regularization_loss: 0.0000e+00 - total_loss: 4.7596\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.8873 - root_mean_squared_error: 3.6716 - loss: 4.7468 - regularization_loss: 0.0000e+00 - total_loss: 4.7468\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.8907 - root_mean_squared_error: 3.6568 - loss: 4.7299 - regularization_loss: 0.0000e+00 - total_loss: 4.7299\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.8940 - root_mean_squared_error: 3.6332 - loss: 4.7067 - regularization_loss: 0.0000e+00 - total_loss: 4.7067\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 1s 129ms/step - ndcg_metric: 0.8968 - root_mean_squared_error: 3.5966 - loss: 4.6784 - regularization_loss: 0.0000e+00 - total_loss: 4.6784\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 1s 116ms/step - ndcg_metric: 0.8995 - root_mean_squared_error: 3.5458 - loss: 4.6491 - regularization_loss: 0.0000e+00 - total_loss: 4.6491\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 1s 119ms/step - ndcg_metric: 0.9020 - root_mean_squared_error: 3.4750 - loss: 4.6231 - regularization_loss: 0.0000e+00 - total_loss: 4.6231\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9040 - root_mean_squared_error: 3.3964 - loss: 4.5990 - regularization_loss: 0.0000e+00 - total_loss: 4.5990\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 1s 121ms/step - ndcg_metric: 0.9057 - root_mean_squared_error: 3.3274 - loss: 4.5787 - regularization_loss: 0.0000e+00 - total_loss: 4.5787\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 1s 115ms/step - ndcg_metric: 0.9071 - root_mean_squared_error: 3.2640 - loss: 4.5650 - regularization_loss: 0.0000e+00 - total_loss: 4.5650\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 1s 119ms/step - ndcg_metric: 0.9082 - root_mean_squared_error: 3.2130 - loss: 4.5515 - regularization_loss: 0.0000e+00 - total_loss: 4.5515\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9090 - root_mean_squared_error: 3.1600 - loss: 4.5384 - regularization_loss: 0.0000e+00 - total_loss: 4.5384\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 1s 118ms/step - ndcg_metric: 0.9098 - root_mean_squared_error: 3.1097 - loss: 4.5321 - regularization_loss: 0.0000e+00 - total_loss: 4.5321\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 1s 119ms/step - ndcg_metric: 0.9104 - root_mean_squared_error: 3.0647 - loss: 4.5231 - regularization_loss: 0.0000e+00 - total_loss: 4.5231\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9109 - root_mean_squared_error: 3.0426 - loss: 4.5154 - regularization_loss: 0.0000e+00 - total_loss: 4.5154\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 1s 115ms/step - ndcg_metric: 0.9113 - root_mean_squared_error: 3.0117 - loss: 4.5121 - regularization_loss: 0.0000e+00 - total_loss: 4.5121\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9116 - root_mean_squared_error: 2.9923 - loss: 4.5071 - regularization_loss: 0.0000e+00 - total_loss: 4.5071\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 1s 117ms/step - ndcg_metric: 0.9118 - root_mean_squared_error: 2.9746 - loss: 4.5028 - regularization_loss: 0.0000e+00 - total_loss: 4.5028\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 1s 116ms/step - ndcg_metric: 0.9121 - root_mean_squared_error: 2.9584 - loss: 4.5013 - regularization_loss: 0.0000e+00 - total_loss: 4.5013\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 1s 120ms/step - ndcg_metric: 0.9124 - root_mean_squared_error: 2.9377 - loss: 4.5015 - regularization_loss: 0.0000e+00 - total_loss: 4.5015\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 1s 124ms/step - ndcg_metric: 0.9125 - root_mean_squared_error: 2.9273 - loss: 4.4961 - regularization_loss: 0.0000e+00 - total_loss: 4.4961\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 1s 130ms/step - ndcg_metric: 0.9127 - root_mean_squared_error: 2.9147 - loss: 4.4948 - regularization_loss: 0.0000e+00 - total_loss: 4.4948\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 1s 120ms/step - ndcg_metric: 0.9130 - root_mean_squared_error: 2.9116 - loss: 4.4930 - regularization_loss: 0.0000e+00 - total_loss: 4.4930\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 1s 121ms/step - ndcg_metric: 0.9131 - root_mean_squared_error: 2.9041 - loss: 4.4927 - regularization_loss: 0.0000e+00 - total_loss: 4.4927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x210e4157610>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "listwise_model.fit(cached_train, epochs=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot iterate over a Tensor with unknown first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 102\u001b[0m\n\u001b[0;32m     98\u001b[0m       tensor_slices[\u001b[39m\"\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(sampled_ratings)\n\u001b[0;32m    100\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(tensor_slices)\n\u001b[1;32m--> 102\u001b[0m test \u001b[39m=\u001b[39m sample_listwise(x, \u001b[39m1\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m42\u001b[39;49m)\n\u001b[0;32m    103\u001b[0m \u001b[39mprint\u001b[39m(test)\n",
      "Cell \u001b[1;32mIn[67], line 74\u001b[0m, in \u001b[0;36msample_listwise\u001b[1;34m(rating_dataset, num_list_per_user, num_examples_per_list, seed)\u001b[0m\n\u001b[0;32m     71\u001b[0m example_lists_by_user \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mdefaultdict(_create_feature_dict)\n\u001b[0;32m     73\u001b[0m movie_title_vocab \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m---> 74\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m rating_dataset:\n\u001b[0;32m     75\u001b[0m   user_id \u001b[39m=\u001b[39m example[\u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     76\u001b[0m   example_lists_by_user[user_id][\u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     77\u001b[0m       example[\u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Marco\\Workspace\\phase-2\\venv\\Lib\\site-packages\\keras\\src\\engine\\keras_tensor.py:410\u001b[0m, in \u001b[0;36mKerasTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a scalar.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a Tensor with unknown first dimension.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m \u001b[39mreturn\u001b[39;00m _KerasTensorIterator(\u001b[39mself\u001b[39m, shape[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."
     ]
    }
   ],
   "source": [
    "\n",
    "import array\n",
    "import collections\n",
    "\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "\n",
    "def _create_feature_dict() -> Dict[Text, List[tf.Tensor]]:\n",
    "  \"\"\"Helper function for creating an empty feature dict for defaultdict.\"\"\"\n",
    "  return {\"encoder\": [], \"rank\": []}\n",
    "\n",
    "def _sample_list(\n",
    "    feature_lists: Dict[Text, List[tf.Tensor]],\n",
    "    num_examples_per_list: int,\n",
    "    random_state: Optional[np.random.RandomState] = None,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Function for sampling a list example from given feature lists.\"\"\"\n",
    "  if random_state is None:\n",
    "    random_state = np.random.RandomState()\n",
    "\n",
    "  sampled_indices = random_state.choice(\n",
    "      range(len(feature_lists[\"encoder\"])),\n",
    "      size=num_examples_per_list,\n",
    "      replace=False,\n",
    "  )\n",
    "  sampled_movie_titles = [\n",
    "      feature_lists[\"encoder\"][idx] for idx in sampled_indices\n",
    "  ]\n",
    "  sampled_ratings = [\n",
    "      feature_lists[\"rank\"][idx]\n",
    "      for idx in sampled_indices\n",
    "  ]\n",
    "\n",
    "  return (\n",
    "      tf.stack(sampled_movie_titles, 0),\n",
    "      tf.stack(sampled_ratings, 0),\n",
    "  )\n",
    "\n",
    "def sample_listwise(\n",
    "    rating_dataset: tf.data.Dataset,\n",
    "    num_list_per_user: int = 10,\n",
    "    num_examples_per_list: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tf.data.Dataset:\n",
    "  \"\"\"Function for converting the MovieLens 100K dataset to a listwise dataset.\n",
    "\n",
    "  Args:\n",
    "      rating_dataset:\n",
    "        The MovieLens ratings dataset loaded from TFDS with features\n",
    "        \"movie_title\", \"user_id\", and \"user_rating\".\n",
    "      num_list_per_user:\n",
    "        An integer representing the number of lists that should be sampled for\n",
    "        each user in the training dataset.\n",
    "      num_examples_per_list:\n",
    "        An integer representing the number of movies to be sampled for each list\n",
    "        from the list of movies rated by the user.\n",
    "      seed:\n",
    "        An integer for creating `np.random.RandomState`.\n",
    "\n",
    "  Returns:\n",
    "      A tf.data.Dataset containing list examples.\n",
    "\n",
    "      Each example contains three keys: \"user_id\", \"movie_title\", and\n",
    "      \"user_rating\". \"user_id\" maps to a string tensor that represents the user\n",
    "      id for the example. \"movie_title\" maps to a tensor of shape\n",
    "      [sum(num_example_per_list)] with dtype tf.string. It represents the list\n",
    "      of candidate movie ids. \"user_rating\" maps to a tensor of shape\n",
    "      [sum(num_example_per_list)] with dtype tf.float32. It represents the\n",
    "      rating of each movie in the candidate list.\n",
    "  \"\"\"\n",
    "  random_state = np.random.RandomState(seed)\n",
    "\n",
    "  example_lists_by_user = collections.defaultdict(_create_feature_dict)\n",
    "\n",
    "  movie_title_vocab = set()\n",
    "  for example in rating_dataset:\n",
    "    user_id = example[\"dataset\"].numpy()\n",
    "    example_lists_by_user[user_id][\"encoder\"].append(\n",
    "        example[\"encoder\"])\n",
    "    example_lists_by_user[user_id][\"rank\"].append(\n",
    "        example[\"rank\"])\n",
    "    movie_title_vocab.add(example[\"encoder\"].numpy())\n",
    "\n",
    "  tensor_slices = {\"dataset\": [], \"encoder\": [], \"rank\": []}\n",
    "\n",
    "  for user_id, feature_lists in example_lists_by_user.items():\n",
    "    for _ in range(num_list_per_user):\n",
    "\n",
    "      # Drop the user if they don't have enough ratings.\n",
    "      if len(feature_lists[\"encoder\"]) < num_examples_per_list:\n",
    "        continue\n",
    "\n",
    "      sampled_movie_titles, sampled_ratings = _sample_list(\n",
    "          feature_lists,\n",
    "          num_examples_per_list,\n",
    "          random_state=random_state,\n",
    "      )\n",
    "      tensor_slices[\"dataset\"].append(user_id)\n",
    "      tensor_slices[\"encoder\"].append(sampled_movie_titles)\n",
    "      tensor_slices[\"rank\"].append(sampled_ratings)\n",
    "\n",
    "  return tf.data.Dataset.from_tensor_slices(tensor_slices)\n",
    "\n",
    "test = sample_listwise(x, 1, 5, 42)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
